1. Consider the following argument:
It really does not matter which of the algorithms {next, worst, first, best} fit is used for memory
placement. Eventually, the system will reach an equilibrium and all the algorithms will perform
similarly.
Is this true? Explain and justify your answer.

No the algorithms perform differently even over the long term. Assuming your inputs are small enough that many can utilize the memory.

A small sample of my test results are below:

Total Memory Size = 1000mb  , Number of Processes: 5000

Mem size 1..50      first   |  best   |  worst  | next
Ave Mem Usage    |   81.1   |  79.2   |  52.4   |  63.4
Ave No.Processes |   383.8  |  351.8  |  224.4  |  244
Ave No. Holes    |   46.7   |  37.3   |  53.3   |  54.8

Mem size 1..400     first   |  best   |  worst  | next
Ave Mem Usage    |   79.9   |  65.8   |  53.3   |  62.3
Ave No.Processes |   56.8   |  56.8   |   33    |  33
Ave No. Holes    |   17.1   |  15.3   |   8.3   |  10.8



From the data above (and other data collected), it can be stated that worst & next fit perform the worse, having both have poor memory utilization.
Worst fit performs badly possibly due to its tendency to leave large holes.
Surprisingly next fit also has poor performance considering first fit has the best performance. This poor performance could be attributed to it leaving holes all throughout memory and not giving them the opportunity to merge.

Best fit is the next worst performer, performing well but not as good as first fit with larger data items.
First fit was the most efficient algorithm. Despite causing many small holes at the start of the free list perhaps this allowed them to merge easily and keep the list relatively organized over the long run.

My other test results included memory with sizes 1..100, 1..200 through to 1..1000, and also confirmed the previous results. The previous results just demonstrated when each of the algorithms were more effective/worse. 


2. (a challenging question) Let the number of segments of memory containing processes be Sm. Let
the number of segments that containing holes be SH. Show on that, SH = Sm รท 2. List any
assumptions you have made.

Assumptions:
All free memory if adjacent is merged
Memory with processes do not merge if adjacent
The memory allocation has been running for some time 

Basically:
For each Sm it could either be next to another Sm or SH.
For each SH it must be next to a process(Sm)

From a tree diagram (given both initially SH & Sm have an equal chance to occur ) we have the following options
So Pr(Sm|SH) = 1/2
&
Pr(Sm|Sm) + Pr(SH|Sm) = 1/2

These are the only 3 outcomes possible SH / Sm / Sm. So over time for every SH there will also be two Sm.
Thus SH = Sm / 2.